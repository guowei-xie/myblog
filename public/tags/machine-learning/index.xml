<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine-Learning on 浑身蟹数</title>
    <link>https://www.xiebro.cool/tags/machine-learning/</link>
    <description>Recent content in Machine-Learning on 浑身蟹数</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 25 May 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://www.xiebro.cool/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>KL散度模拟</title>
      <link>https://www.xiebro.cool/post/kullback-leibler/</link>
      <pubDate>Sat, 25 May 2024 00:00:00 +0000</pubDate>
      <guid>https://www.xiebro.cool/post/kullback-leibler/</guid>
      <description>从采样角度出发对KL散度进行直观解释,即：KL散度描述了我们用分布Q来估计数据的真实分布P的信息(编码)损失。&#xA;附：一篇关于Kullback-Leibler Divergence 基本介绍的blog&#xA;验证问题 假如真实样本P已知，Q是对P的随机采样样本，那么Q相对于P的信息损失是否与采样样本量呈线性关系？&#xA;换句话讲，假如Q的样本量与P相等，那么Q可以100%程度表述P的信息，假如Q的样本量是P的10%呢？此时是否会相较于前者损失90%的信息？即KL散度与样本量是否是线性的关系？&#xA;模拟验证 library(tidyverse) library(furrr) library(hrbrthemes) theme_set(theme_ipsum(base_family = &amp;#34;Kai&amp;#34;, base_size = 8)) plan(multisession, workers = 10) set.seed(42) # 模拟总体样本P P &amp;lt;- rbinom(10000, 1, 0.1) cat(paste0(&amp;#34;总体样本P的样本量：&amp;#34;, length(P),&amp;#34;, 均值:&amp;#34;, mean(P))) ## 总体样本P的样本量：10000, 均值:0.1031 # bootstrap估计P的均值分布 trials &amp;lt;- 10000 P_norm &amp;lt;- 1 : trials |&amp;gt; future_map_dbl(~ { P |&amp;gt; sample(replace = TRUE) |&amp;gt; mean() }) cat(paste0(&amp;#34;P的均值分布参数: μ = &amp;#34;, mean(P_norm), &amp;#34;, σ = &amp;#34;, sd(P_norm))) ## P的均值分布参数: μ = 0.</description>
    </item>
    <item>
      <title>异常值检测</title>
      <link>https://www.xiebro.cool/post/anomaly-detection/</link>
      <pubDate>Sun, 24 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://www.xiebro.cool/post/anomaly-detection/</guid>
      <description>library(tidyverse) library(hrbrthemes) theme_set(theme_ipsum(base_family = &amp;#34;Kai&amp;#34;, base_size = 8)) Dummy Data with Outliers set.seed(1234) N = 1e3 x = c(rnorm(N, 0, 0.5), rnorm(N * 0.05, -1, 1)) y = c(rnorm(N, 0, 0.5), rnorm(N * 0.05, 1, 1)) o = c(rep(0L, N), rep(1L, (N * 0.05))) mm &amp;lt;- matrix(x, y, nrow = N * 1.05, ncol = 2) dat &amp;lt;- tibble(x, y, outlier = o) dat |&amp;gt; ggplot(aes(x, y, col = factor(o))) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + geom_point(alpha = 0.</description>
    </item>
  </channel>
</rss>
